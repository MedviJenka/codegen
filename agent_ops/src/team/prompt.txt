"""Your task is to compare summaries generated by different large language models (LLMs)
from a meeting transcript.Here is the first summary from engine {engine1_name}:<summary1>{summary1}</summary1>
And here is the second summary from engine {engine2_name}:<summary2>{summary2}</summary2>The original transcript is
as follows:<transcript>{transcript}</transcript>Your goal is to compare the above summaries across various
scientific parameters to assess their relative quality. The key parameters to evaluate are:- **Factual accuracy**:
Assess if there are any incorrect or unsupported claims.- **Conciseness**: Evaluate how much relevant versus irrelevant
detail is included.- **Coherence**: Determine if there is a logical flow and smooth transitions between ideas.-
**Abstractedness**: Check the use of novel words and phrases not present in the transcript. Instructions:1.
Compare each summary to the original transcript:
- Identify factual inaccuracies.
- Assess conciseness for relevance versus unnecessary details.
- Analyze the logical flow and coherence.
- Determine abstractedness by calculating the percentage of novel versus verbatim content.
- if the summaries are not in English, but keep english words in the other language summary ,
for example : אס בי סי was kept SBC, grant more points to that summary.
we want to keep important words like name, companies and producs in English,
although the summary is not in English language.
2. Assign numerical scores (1-10) for each parameter for both summaries.
3. Provide an overall score for each summary based on the parameter scores.
4. Embed specific explanations for each parameter as part of the JSON response.
Output Requirement:The output **must be strictly in the following JSON format**.
Return only the JSON—no additional text, explanations, or comments outside the JSON structure.
JSON Response Format:```json{{  "meeting_id": "{meeting_id}",  "parameters": {{    "{engine1_name}": {{
"factual_accuracy": <number>,      "conciseness": <number>,      "coherence": <number>,
"abstractedness": <number>,      "overall_score": <number>    }},    "{engine2_name}": {{
"factual_accuracy": <number>,      "conciseness": <number>,      "coherence": <number>,      "abstractedness": <number>,
"overall_score": <number>    }}  }},  "explanations": {{    "factual_accuracy": "<text explaining the factual accuracy scores>",
"conciseness": "<text explaining the conciseness scores>",    "coherence": "<text explaining the coherence scores>",
"abstractedness": "<text explaining the abstractedness scores>",
"overall_scores": "<text explaining the overall scores>"  }}}}"""
